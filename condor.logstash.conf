# This processes an HTCondor EventLog and sends the events to Elasticsearch.
# It requires that a Job ad information event (028) be triggered with
# supplemental classad information via the JobAdInformationAttrs config val.
# see http://research.cs.wisc.edu/htcondor/manual/current/12_Appendix_A.html#JobAdInformationAttrs-job-attribute
#
# Requires aggregate plugin https://www.elastic.co/guide/en/logstash/current/plugins-filters-aggregate.html
input {
    file {
        path => "/var/log/condor/EventLog"
        type => "condor_eventlog"
        start_position => beginning
        delimiter => "
...
"
        codec => multiline {
            pattern => "^[^\d]"
            what => "previous"
        }
        add_field => { "pool" => "fifebatch" }
    }
}
filter {
    if [type] == "condor_eventlog" {
        grok {
            patterns_dir => "/etc/landscape/patterns"
            match => {
                 "message" => [
                    "%{CONDOR_EVENT:event} %{DATA:event_message}\n%{GREEDYDATA:event_body}\n\.\.\.",
                    "%{CONDOR_EVENT:event} %{DATA:event_message}\n\.\.\."
                ]
            }
        }
        date {
            match => [ "condor_timestamp", "MM/dd HH:mm:ss" ]
            remove_field => [ "condor_timestamp" ]
        }
        mutate {
            add_field => { "jobid" => "%{cluster}.%{process}@%{host}" }
        }
        truncate {
            fields => [ "ErrorMsg" ]
            length_bytes => 16384
        }
        if [event_code] == "028" {
            aggregate {
                task_id => "%{cluster}.%{process}.%{subprocess}@%{host}"
                code => "event.set('trigger_event_message',map['trigger_event_message'])"
                map_action => "update"
                end_of_task => true
                timeout => "60"
            }
            kv {
                source => "event_body"
                value_split => " = "
                field_split => "\n"
                remove_field => [ "event_body" ]
            }
            mutate {
                convert => {
                    "MemoryUsage" => "integer"
                    "ResidentSetSize" => "integer"
                    "ReceivedBytes" => "float"
                    "SentBytes" => "float"
                    "TotalReceivedBytes" => "float"
                    "TotalSentBytes" => "float"
                    "Size" => "integer"
                 }
            }
            ruby {
                code => '
                    # convert to bytes
                    if event.include?("MemoryUsage")
                        event.set("MemoryUsage",event.get("MemoryUsage").to_i*1024*1024)
                    end
                    if event.include?("ResidentSetSize")
                        event.set("ResidentSetSize", event.get("ResidentSetSize").to_i*1024)
                    end'
            }
            mutate {
                gsub => [
                    "trigger_event_message", "\n", " ",
                    "trigger_event_message", "\t", " "
                ]
            }
            grok {
                patterns_dir => "/etc/landscape/patterns"
                match => {
                     "trigger_event_message" => [
                        "%{CONDOR_EVENT_001}",
                        "%{CONDOR_EVENT_006}",
                        "%{CONDOR_EVENT_012}",
                        "%{CONDOR_EVENT_005_RESOURCES}",
                        "%{CONDOR_EVENT_005}",
                        "%{CONDOR_EVENT:event} %{DATA:event_message}\n%{GREEDYDATA:_trigger_event_body}",
                        "%{CONDOR_EVENT:event} %{DATA:event_message}"
                    ]
                }
                overwrite => [ "event", "event_message", "event_code", "cluster", "process", "subprocess" ]
                remove_field => [ "trigger_event_message" ]
            }
            # combine durations
            if [event_code] == "005" {
                ruby {
                    code => '
                        for metric in ["local_sys_time","local_user_time","remote_sys_time","remote_user_time"] do
                            for group in ["run","total"] do
                                name = group+"_"+metric
                                time = event.get(name+"_seconds") || 0
                                event.remove(name+"_seconds")
                                time += (event.get(name+"_minutes") || 0) * 60
                                event.remove(name+"_minutes")
                                time += (event.get(name+"_hours") || 0) * 3600
                                event.remove(name+"_hours")
                                time += (event.get(name+"_days") || 0) * 3600 * 24
                                event.remove(name+"_days")
                                event.set(name,time)
                            end
                        end'
                }
            }
        } else {
            aggregate {
                task_id => "%{cluster}.%{process}.%{subprocess}@%{host}"
                code => "map['trigger_event_message']=event.get('message')"
                map_action => "create"
            }
            drop {}
        }
    }
}
output {
    elasticsearch {
        hosts => [ "fifemon-es.fnal.gov" ]
        ssl => true
        cacert => "/etc/grid-security/certificates/cilogon-osg.pem"
        keystore => "/fife/local/home/rexbatch/fifemon-logstash/logstash_fifebatch_fnal_gov.p12"
        keystore_password => ""
        index => "fifebatch-logs-%{+YYYY.MM.dd}"
        template_name => "fifebatch-logs"
        template => "/fife/local/home/rexbatch/fifemon-logstash/fifebatch-logs.json"
        manage_template => true
        template_overwrite => true
    }
    #stdout {codec => "rubydebug"}
}
